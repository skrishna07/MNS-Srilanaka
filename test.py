from openai import OpenAI
client = OpenAI()

response = client.chat.completions.create(
  model="gpt-4o",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "import pandas as pd import re import json # Import the json module\\ def srilanka_mapping_and_comp(output, excel_file_path, config_file_path, json_file_path, is_pnl): \"\"\" config_file_path : Italian Financial Config excel_file_path : PDF to Excel converted file. \"\"\" print(\"italian mapping an comp called\") print(\"is_pnl\",is_pnl) european_number_regex = r'^\\d{1,3}(,\\d{3})*$' date_regex = r\"\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}\" parentheses_number_regex = r\"\\(\\d{1,3}(,\\d{3})*\\)\" # Load the financial config config_df = pd.read_excel(config_file_path) excel_nodes_list = [] # Load the PDF to Excel converted file (to get all sheets) excel_sheets = pd.ExcelFile(excel_file_path) def safe_clean_field(field): # Check if the field is None or NaN (using float('nan')) if field is None or (isinstance(field, float) and field != field): # field != field checks for NaN return \"\" return str(field).strip().lower() def preprocess_field(field): if not isinstance(field, str): if re.match(european_number_regex, str(field)): return f'{field}' # Add quotes if it matches the regex else: return \"\" # Return an empty string if it doesn't match cleaned_field = field.strip() cleaned_field = re.sub(r\"^-+\\s*|\\n:unselected:|:unselected:\\n\",'', cleaned_field).strip() cleaned_field = cleaned_field.strip('-').strip() return cleaned_field # Define the headers for each category non_current_assets_headers = [\"Non-current assets\", \"Non current activities\", \"FIXED ASSETS\"] current_assets_headers = [\"Current assets\", \"Current activities\"] equity_related_headers = [\"EQUITY\"] non_current_liabilities_headers = [\"Non-current liabilities\", \"Non current liabilities\"] current_liabilities_headers = [\"Current liabilities\"] total_assets_headers = [\"TOTAL ACTIVITIES\",\"Total assets\", \"Activity\"] total_liabilities_headers = [\"Total Net Equity and Liabilities\",\"Total LiabiLities and sharehoLders' Equity\",\"Total shareholders' equity and liabilities\", \"Shareholders' equity and liabilities\", \"Total liabilities\"] fields_to_ignore = [\"non-current assets\",\"Non-current assets\", \"non current activities\", \"fixed assets\", \"current assets\", \"current activities\", \"equity\", \"non-current liabilities\", \"current liabilities\", \"total activities\", \"total assets\", \"activity\", \"total net equity and liabilities\", \"total liabilities and shareholders' equity\", \"total shareholders' equity and liabilities\", \"shareholders' equity and liabilities\", \"total liabilities\", \"liabilities and shareholders' equity\", \"total non-current assets\", \"total current assets\", \"total non-current liabilities\", \"total current liabilities\", \"total equity\", \"net assets\", \"Non current liabilities\", \"LIABILITIES\",\"Total equity and liabilities\" ] fields_to_ignore = [field.lower().strip() for field in fields_to_ignore] def update_both_entities_data(entity, node_with_values, is_standalone_first, main_dict_node): print(\"entitytt\", entity) # Determine which values to use based on `is_standalone_first` for key, value in node_with_values.items(): if is_standalone_first: if entity == 'Company': current_year_value, previous_year_value = value[0], value[1] elif entity == 'Group': current_year_value, previous_year_value = value[2], value[3] else: if entity == 'Group': current_year_value, previous_year_value = value[0], value[1] elif entity == 'Company': current_year_value, previous_year_value = value[2], value[3] print(f\"curr {current_year_value} and prev {previous_year_value}\") # Retrieve existing values from the `output` dictionary openai_current_value = ( output.get(entity, [{}])[0] .get(current_year, {}) .get(main_dict_node, {}) .get(key, None) ) openai_previous_value = ( output.get(entity, [{}])[0] .get(previous_year, {}) .get(main_dict_node, {}) .get(key, None) if previous_year is not None else None ) # Update `output` if values are different if openai_current_value != current_year_value: if main_dict_node is None: output.setdefault(entity, [{}])[0].setdefault(current_year, {})[ key ] = current_year_value else: output.setdefault(entity, [{}])[0].setdefault(current_year, {}).setdefault( main_dict_node, {} )[key] = current_year_value if previous_year and openai_previous_value != previous_year_value: if main_dict_node is None: output.setdefault(entity, [{}])[0].setdefault(previous_year, {})[ key ] = previous_year_value else: output.setdefault(entity, [{}])[0].setdefault(previous_year, {}).setdefault( main_dict_node, {} )[key] = previous_year_value return output def compare_openai_and_Excel_data(node_list, fields_data, main_dict_node, both_entities_found, is_standalone_first): # print(\"fields_data\", fields_data) print(\"both_entities_found\",both_entities_found) node_with_values = {} for node in node_list: # print(\"node\",node) node_found = False node_values = [] # Reset node values for each node nodes_not_to_keep = [] for idx, field in enumerate(fields_data): # Check if the node matches the field with a similarity threshold if str(node).lower() == preprocess_field(field).lower(): print(\"Nodeeeeeeee\", node) print(f\"Node '{node}' found at index {idx} in the fields_data.\") # Current year value is the next value after the node if idx + 1 < len(fields_data): current_year_value = fields_data[idx + 1] if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(current_year_value)): current_year_value = \"\" if pd.isna(current_year_value) or current_year_value == '-' or current_year_value == 'nan': current_year_value = '' node_values.append(current_year_value) # Ensure current_year_value is always a string before further processing current_year_value = str(current_year_value) # Check and process values with parentheses if current_year_value.startswith('(') and current_year_value.endswith(')'): current_year_value = current_year_value[1:-1] # Remove brackets # Match the value against the regex match = re.search(european_number_regex, current_year_value) if match or current_year_value == '': if current_year_value != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') node_values.append(current_year_value) # Previous year value can be extracted similarly, if available if idx + 2 < len(fields_data): previous_year_value = fields_data[idx + 2] print(f\"Previous Year Value: {previous_year_value}\") if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(previous_year_value)): previous_year_value = \"\" if pd.isna(previous_year_value) or previous_year_value == '-' or previous_year_value == 'nan': previous_year_value = '' node_values.append(previous_year_value) # Ensure current_year_value is always a string before further processing previous_year_value = str(previous_year_value) if previous_year_value.startswith('(') and previous_year_value.endswith(')'): previous_year_value = previous_year_value[1:-1] print(f\"AFTER PREVIOUS Year Value: {previous_year_value}\") match = re.search(european_number_regex, str(previous_year_value)) if match or previous_year_value == '': if previous_year_value != '': node_values.append(previous_year_value) print(\"type\",both_entities_found) if both_entities_found: if idx + 3 < len(fields_data): current_year_value_1 = fields_data[idx + 3] if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(current_year_value_1)): current_year_value_1 = \"\" print(f\"Current Year Value: {current_year_value_1}\") if pd.isna(current_year_value_1) or current_year_value_1 == '-' or current_year_value_1 == 'nan': current_year_value_1 = '' node_values.append(current_year_value_1) # Ensure current_year_value is always a string before further processing current_year_value_1 = str(current_year_value_1) # Check and process values with parentheses if current_year_value_1.startswith('(') and current_year_value_1.endswith(')'): current_year_value_1 = current_year_value_1[1:-1] # Remove brackets # Match the value against the regex match = re.search(european_number_regex, current_year_value_1) if match or current_year_value_1 == '': if current_year_value_1 != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') node_values.append(current_year_value_1) if idx + 4 < len(fields_data): previous_year_value_1 = fields_data[idx + 4] print(f\"Previous Year Value: {previous_year_value_1}\") if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(previous_year_value_1)): previous_year_value_1 = \"\" if pd.isna(previous_year_value_1) or previous_year_value_1 == '-' or previous_year_value_1 == 'nan': print(\"yeesss\") previous_year_value_1 = '' node_values.append(previous_year_value_1) # Ensure current_year_value is always a string before further processing previous_year_value_1 = str(previous_year_value_1) # if isinstance(previous_year_value, float): # value = str(previous_year_value) # Convert float to string for consistent processing if previous_year_value_1.startswith('(') and previous_year_value_1.endswith(')'): previous_year_value_1 = previous_year_value_1[1:-1] match = re.search(european_number_regex, str(previous_year_value_1)) if match or previous_year_value_1 == '': if previous_year_value_1 != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') node_values.append(previous_year_value_1) node_found = True node_with_values[node] = node_values print(\"node_with_values\", node_with_values) break # Break the inner loop as we found the node if node_found and not both_entities_found: excel_nodes_list.append(node) print(f\"Extracted values for '{node}': {node_values}\") if len(node_values) >= 1: current_year_value = node_values[0] previous_year_value = node_values[1] if len(node_values) > 1 else None # Check if the node is in the total_assets_list if any(total_asset_keyword.lower() == node.lower() for total_asset_keyword in total_assets_list): print(f\"Node '{node}' is in total_assets_list. Updating the values.\") output[entity][0][current_year][node] = current_year_value if previous_year: output[entity][0][previous_year][node] = previous_year_value continue # Skip the rest of the logic for this node # Handle OpenAI current and previous values openai_current_value = output.get(entity, [{}])[0].get(current_year, {}).get(main_dict_node, {}).get(node, None) openai_previous_value = None if previous_year is not None: openai_previous_value = output.get(entity, [{}])[0].get(previous_year, {}).get( main_dict_node, {}).get(node, None) if openai_current_value != current_year_value or openai_previous_value != previous_year_value: if openai_current_value != current_year_value: if main_dict_node is None: output[entity][0][current_year][node] = current_year_value else: output[entity][0][current_year].setdefault(main_dict_node, {})[ node] = current_year_value if previous_year and openai_previous_value != previous_year_value: if main_dict_node is None: output[entity][0][previous_year][node] = previous_year_value else: output[entity][0][previous_year].setdefault(main_dict_node, {})[ node] = previous_year_value else: if current_year_value: if main_dict_node is None: output[entity][0][current_year][node] = current_year_value else: output[entity][0][current_year].setdefault(main_dict_node, {})[ node] = current_year_value if previous_year_value and previous_year is not None: if main_dict_node is None: output[entity][0][previous_year][node] = previous_year_value else: output[entity][0][previous_year].setdefault(main_dict_node, {})[ node] = previous_year_value print(\"excel_nodes_list\", excel_nodes_list) else: if both_entities_found: if entity == 'Group': update_both_entities_data(entity, node_with_values, is_standalone_first, main_dict_node) elif entity == 'Company': update_both_entities_data(entity, node_with_values, is_standalone_first, main_dict_node) return output, excel_nodes_list def remove_duplicate_nodes_from_final_output(output, config_df, fields_data, is_pnl): segregated_nodes = { 'Non-current assets': {}, 'Current assets': {}, 'EQUITY': {}, 'Non-current liabilities': {}, 'Current liabilities': {}, 'Profit and Loss': {} } special_nodes = [ \"Total assets\", 'TOTAL ACTIVITIES', \"Activity\", \"Total shareholders' equity and liabilities\", \"Total LiabiLities and sharehoLders' Equity\", 'Total Net Equity and Liabilities', \"Shareholders' equity and liabilities\", \"Total liabilities\" ] # Iterate through each row in config_df for _, row in config_df.iterrows(): main_dict_node = row['main_dict_node'] node_type = row['Type'] node = row['Node'] column_name = row['Column_Name'] group = row['Group'] # Skip invalid groups if pd.isna(group) or group == 'nan': continue # Special handling for special nodes if node in special_nodes: if \"\" not in segregated_nodes: segregated_nodes[\"\"] = {} if group not in segregated_nodes[\"\"]: segregated_nodes[\"\"][group] = [] segregated_nodes[\"\"][group].append(node) elif is_pnl and column_name == 'financials_pnl_lineitems': if node_type == 'Straight' and node != \"Null\": segregated_nodes.setdefault(\"Profit and Loss\", {}) segregated_nodes[\"Profit and Loss\"].setdefault(group, []) segregated_nodes[\"Profit and Loss\"][group].append(node) elif not is_pnl and node_type == 'Straight' and node != \"Null\": if main_dict_node in segregated_nodes: segregated_nodes[main_dict_node].setdefault(group, []) segregated_nodes[main_dict_node][group].append(node) # Processing segregated_nodes for exact and partial matches in Excel for main_node, groups in segregated_nodes.items(): # print(f\"Main Node: {main_node}\") for group, nodes in groups.items(): # print(\"Group\", groups) # print(\"nodes\",nodes) node_values = [] nodes_to_keep = [] for node in nodes: node_found = False node_index = None # Reset for each node for idx, field in enumerate(fields_data): if str(node).lower() == preprocess_field(field).lower(): node_found = True node_index = idx break # Exit the loop once a match is found # print(\"node_index\", node_index) # Only proceed if a match is found if node_index is not None: value_indices = [node_index + 1, node_index + 2] # Loop through the value indices for i, value_idx in enumerate(value_indices): if value_idx < len(fields_data): value = fields_data[value_idx] if pd.isna(value): # Check if value is NaN value = \"\" # Assign an empty string if value.startswith('(') and value.endswith(')'): value = value[1:-1] # Check for date format and assign an empty string if matched if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(value)): print(\"yesssssssssss\") value = \"\" node_values.append(value) print(\"Node_values\", node_values) print(f\"Value {i + 1}: {value}\") if re.match(european_number_regex, value): # Validate with regex node_values.append(value) if node_values!= [] and node_values[0] == '' and node_values[1] == '': print(\"Node\",node) node_values = [] if node_found and len(node_values)>0: print(\"Node_found\", node) node_found_in_excel = True nodes_to_keep.append(node) # Add this node to the nodes to keep excel_nodes_list.append(node) print(\"nodes_to_keep\", nodes_to_keep) # Now, update the output and remove any nodes not found in Excel for group, nodes in groups.items(): # For each group, remove nodes that are not found in Excel # print(f\"group {group} and nodes {nodes}\") for node in nodes: # print(\"for node in nodes\", node) if node in nodes_to_keep: print( f\" Node: {node} found in Excel, removing other nodes from the group {group}\") # Loop over both current and previous year sections and remove other nodes year_data = {} for year in [current_year, previous_year]: if main_node == \"Profit and Loss\" or main_node == '': # Directly retrieve year data for Profit and Loss year_data = output.get(entity, [{}])[0].get(year, {}) else: year_data = output.get(entity, [{}])[0].get(year, {}).get(main_node, {}) print(\"year_Data\", year_data) if node in year_data: # Remove all other nodes in this group from the current year's section for other_node in nodes: # print(\"other_node\") if other_node != node and other_node in year_data: print( f\" Removing {other_node} from {main_node} in {year}\") del year_data[other_node] # Remove the other node # After checking and modifying, only keep the nodes that were found in Excel for node in nodes_to_keep: print(f\" Keeping node: {node}\") def check_nodes_in_excel(headers_list, node_list, ignore_headers, main_dict_node, is_pnl=False, all_tags_data=None): global row_start, row_end, excel_nodes_list matched_headers = set() if all_tags_data is None: all_tags_data = [] # Initialize if not provided print(f\"main_dict_node {main_dict_node} and nodes_list {node_list}\") row_end = None row_start = None new_tags_list = [] new_tags_dict = {} standalone_synonyms = [\"Group\"] consolidated_synonyms = [\"Group\",\"Company\"] is_standalone_first = False # Extract values from the specified range fields_data = excel_data_frame.iloc[row_start:row_end].values.flatten() # Initialize flags standalone_found = any(synonym in fields_data for synonym in standalone_synonyms) consolidated_found = any(synonym in fields_data for synonym in consolidated_synonyms) # Find indexes of standalone_synonyms in fields_data # Find the first index of standalone_synonyms in fields_data standalone_index = next( (idx for idx, value in enumerate(fields_data) if value in standalone_synonyms), None ) # Find the first index of consolidated_synonyms in fields_data consolidated_index = next( (idx for idx, value in enumerate(fields_data) if value in consolidated_synonyms), None ) # Check if both entities are found if standalone_found and consolidated_found: both_entities_found = True if standalone_index < consolidated_index: is_standalone_first = True else: is_standalone_first = False print(\"Both standalone and consolidated entities found.\") print(f\"Indexes of standalone synonyms: {standalone_index}\") print(f\"Indexes of consolidated synonyms: {consolidated_index}\") else: both_entities_found = False print(\"Either standalone or consolidated entity is missing.\") if is_pnl: # Assuming excel_data_frame is your dataframe fields_data = excel_data_frame.iloc[row_start:row_end].values.flatten() row_start = 0 row_end = len(fields_data) # Step 1: Find the indices of the fields in `fields_data` fields_data = [str(field).lower().strip() if isinstance(field, str) else str(field) for field in fields_data] indices = [i for i, field in enumerate(fields_data) if field in fields_to_ignore] if indices: # Step 2: Update row_start and row_end based on the first and last indices row_start = indices[0] row_end = indices[-1] + 1 # +1 to include the last index in the range print(\"row_start:\", row_start, \"First field in fields_data:\", fields_data[row_start]) print(\"row_end:\", row_end, \"Last field in fields_data:\", fields_data[indices[-1]]) print(\"data\",fields_data[:row_start]) # Step 3: Delete data within row_start and row_end and store the remaining data fields_data_new = list(fields_data[:row_start]) + list(fields_data[row_end:]) print(\"Updated fields_data:\", fields_data_new) # Initialize the final dictionary before the loop final_tags_dict = {'PnL': []} fields_to_ignore_1 = ['euro', '$', 'economic entity', 'parent entity', 'nan'] # Loop through the fields_data for idx, field in enumerate(fields_data_new): print('field,',field) if ( preprocess_field(field) and # Ensure the field is not empty or None all(node.lower() != preprocess_field(field).lower() for node in node_list) and # Not in node_list preprocess_field(field).lower() not in (ignore.lower() for ignore in fields_to_ignore) and # Not in ignore list not re.search(date_regex, preprocess_field(field)) and # Field doesn't match date format not re.search(european_number_regex, preprocess_field(field)) and # Field doesn't match European number format not re.match(parentheses_number_regex, preprocess_field(field)) and # Field shouldn't be a number in parentheses not pd.isna(preprocess_field(field)) and # Field shouldn't be NaN \"unnamed\" not in preprocess_field(field).lower() and # Field shouldn't contain 'unnamed' preprocess_field(field).lower() not in (ignore.lower() for ignore in fields_to_ignore_1) and not re.match(r\"^\\d{4}$\", preprocess_field(field).strip()) # Field shouldn't be a year (e.g., 2023) ): # Add preprocessed field to the list values_to_add = [preprocess_field(field)] # Check bounds to avoid IndexError for current_year_value if idx + 1 < len(fields_data_new): current_year_value = fields_data_new[idx + 1] print(\"current_year_value\", current_year_value) if pd.isna(current_year_value) or current_year_value == '-' or current_year_value == 'nan': current_year_value = '' values_to_add.append(current_year_value) current_year_value = str(current_year_value) # Check and process values with parentheses if current_year_value.startswith('(') and current_year_value.endswith(')'): current_year_value = current_year_value[1:-1] # Remove brackets # Match the value against the regex match = re.search(european_number_regex, current_year_value) if match or current_year_value == '': if current_year_value != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') values_to_add.append(current_year_value) if idx + 2 < len(fields_data_new): previous_year_value = fields_data_new[idx + 2] print(\"previous_year_value\", previous_year_value) if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(previous_year_value)): previous_year_value = \"\" print(f\"Previous Year Value: {previous_year_value}\") if pd.isna(previous_year_value) or previous_year_value == '-' or previous_year_value == 'nan': previous_year_value = '' values_to_add.append(previous_year_value) previous_year_value = str(previous_year_value) if previous_year_value.startswith('(') and previous_year_value.endswith(')'): previous_year_value = previous_year_value[1:-1] match = re.search(european_number_regex, str(previous_year_value)) if match or previous_year_value == '': if previous_year_value != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') values_to_add.append(previous_year_value) if both_entities_found: if idx + 3 < len(fields_data): current_year_value_1 = fields_data[idx + 3] if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(current_year_value_1)): current_year_value_1 = \"\" print(f\"Current Year Value: {current_year_value_1}\") if pd.isna( current_year_value_1) or current_year_value_1 == '-' or current_year_value_1 == 'nan': current_year_value_1 = '' values_to_add.append(current_year_value_1) # Ensure current_year_value is always a string before further processing current_year_value_1 = str(current_year_value_1) # Check and process values with parentheses if current_year_value_1.startswith('(') and current_year_value_1.endswith(')'): current_year_value_1 = current_year_value_1[1:-1] # Remove brackets # Match the value against the regex match = re.search(european_number_regex, current_year_value_1) if match or current_year_value_1 == '': if current_year_value_1 != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') values_to_add.append(current_year_value_1) print(\"yayyyy\", current_year_value_1) if idx + 4 < len(fields_data): previous_year_value_1 = fields_data[idx + 4] if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(previous_year_value_1)): previous_year_value_1 = \"\" print(f\"Previous Year Value: {previous_year_value_1}\") if pd.isna( previous_year_value_1) or previous_year_value_1 == '-' or previous_year_value_1 == 'nan': previous_year_value_1 = '' values_to_add.append(previous_year_value_1) # Ensure current_year_value is always a string before further processing previous_year_value_1 = str(previous_year_value_1) # if isinstance(previous_year_value, float): # value = str(previous_year_value) # Convert float to string for consistent processing if previous_year_value_1.startswith('(') and previous_year_value_1.endswith(')'): previous_year_value_1 = previous_year_value_1[1:-1] match = re.search(european_number_regex, str(previous_year_value_1)) if match or previous_year_value_1 == '': if previous_year_value_1 != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') values_to_add.append(previous_year_value_1) # Append the values to the final dictionary's 'PnL' list final_tags_dict['PnL'].extend(values_to_add) # After the loop ends, `final_tags_dict` will contain all the values under the 'PnL' key print(\"Final tags data:\", final_tags_dict) # Append the final dictionary to all_tags_data all_tags_data.append(final_tags_dict) main_dict_node = None output, excel_nodes_list = compare_openai_and_Excel_data(node_list, fields_data_new, main_dict_node, both_entities_found, is_standalone_first) remove_duplicate_nodes_from_final_output(output, config_df, fields_data_new, is_pnl) print(\"All tags data:\", all_tags_data) else: print(\"Mainnn_dict_node\", main_dict_node) row_start = 0 row_end = len(excel_data_frame) fields_data = excel_data_frame.iloc[row_start:row_end].values.flatten() fields_data = [str(field).lower().strip() if isinstance(field, str) else str(field) for field in fields_data] indices = [i for i, field in enumerate(fields_data) if field in fields_to_ignore] print(\"indices\", indices) if indices: # Step 2: Update row_start and row_end based on the first and last indices row_start = indices[0] row_end = indices[-1] + 1 # +1 to include the last index in the range first_field = fields_data[row_start] last_field = fields_data[indices[-1]] print(f\"First field in fields_to_ignore: {first_field} and row_start {row_start}\") print(f\"Last field in fields_to_ignore: {last_field} and row_end {row_end}\") fields_data = fields_data[row_start:row_end] print(\"fields_dataaaaaaaaaaaaaaa\",fields_data) fields_to_ignore_1 = ['euro', '$', 'economic entity', 'parent entity', 'nan'] header_row_index = None for header in headers_list: for idx, field in enumerate(fields_data): # print(\"field\", field) # print(\"preprocess_field\", preprocess_field(field).lower()) # print(\"header\", header) if str(header).lower() == preprocess_field(field).lower(): print(f\"Header '{header}' found at index {idx} in fields_data.\") header_row_index = idx break else: # No matching header found, continue to the next field print(f\"No matching row index found for header\") continue # Fetch the next two values in the same row after the header financial_values = [] # To store the financial values found financial_values_found = False if header_row_index is not None: value_indices = [header_row_index + 1, header_row_index + 2] # Loop through the value indices for i, value_idx in enumerate(value_indices): if value_idx < len(fields_data): value = fields_data[value_idx] if pd.isna(value): # Check if value is NaN value = \"\" # Assign an empty string if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(value)): # print(\"yesssssssssss\") value = \"\" print(f\"Value {i + 1}: {value}\") if re.match(european_number_regex, value): # Validate with regex financial_values.append(value) financial_values_found = True print(\"financial_values\",financial_values) if header in total_assets_headers or header in total_liabilities_headers: financial_values_found = False # Print \"Yes\" if financial values are found, otherwise \"No\" if financial_values_found: print(f\"Yes, financial values found for '{header}' in the two-year columns.\") print(f\"Financial values for '{header}': {financial_values}\") potential_row_end_1 = [] row_end = header_row_index # This marks the end of the current section. # Find potential row_end_1 values based on ignore_headers for other_header in ignore_headers: print(\"Checking for other_header in ignore_headers:\", other_header) for idx, field in enumerate(fields_data): if str(other_header).lower() == preprocess_field(field).lower(): print(f\"Potential row_end_2 found at row {idx} for header '{other_header}'\") potential_row_end_1.append(idx) break print(\"Potential row_end_1 indices:\", potential_row_end_1) # Determine the row_start (just before header_row_index) row_start = 0 # Default to 0 if no valid start index is found for idx_1 in sorted(potential_row_end_1): # Sort in ascending order if idx_1 < header_row_index: # Find the largest index smaller than header_row_index row_start = idx_1 + 1 else: break # Stop once we pass header_row_index print(\"Row start determined as:\", row_start) fields_data_new = fields_data[row_start:row_end] for idx, field in enumerate(fields_data_new): if ( preprocess_field(field) and all(node.lower() != preprocess_field(field).lower() for node in node_list) and preprocess_field(field).lower() not in (ignore.lower() for ignore in fields_to_ignore) and # Not in ignore list not re.search(date_regex, preprocess_field(field)) and # Field doesn't match date format not re.search(european_number_regex, preprocess_field(field)) and # Field doesn't match European number format not re.match(parentheses_number_regex, preprocess_field(field)) and not pd.isna(preprocess_field(field)) and # Field shouldn't be NaN \"unnamed\" not in preprocess_field(field).lower() and # Field shouldn't preprocess_field(field).lower() not in (ignore.lower() for ignore in fields_to_ignore_1) and not re.match(r\"^\\d{4}$\",preprocess_field(field).strip()) ): values_to_add = [preprocess_field(field)] print(\"preprocess_field\",values_to_add ) # Check bounds to avoid IndexError if idx + 1 < len(fields_data_new): current_year_value = fields_data_new[idx + 1] if pd.isna(current_year_value) or current_year_value == '-' or current_year_value == 'nan': current_year_value = '' values_to_add.append(current_year_value) # Ensure current_year_value is always a string before further processing current_year_value = str(current_year_value) # Check and process values with parentheses if current_year_value.startswith('(') and current_year_value.endswith(')'): current_year_value = current_year_value[1:-1] # Remove brackets # Match the value against the regex match = re.search(european_number_regex, current_year_value) if match or current_year_value == '': if current_year_value != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace( '#', ',') values_to_add.append(current_year_value) if idx + 2 < len(fields_data_new): previous_year_value = fields_data_new[idx + 2] if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(previous_year_value)): previous_year_value = \"\" print(f\"Previous Year Value: {previous_year_value}\") if pd.isna(previous_year_value) or previous_year_value == '-' or previous_year_value == 'nan': previous_year_value = '' values_to_add.append(previous_year_value) # Ensure current_year_value is always a string before further processing previous_year_value = str(previous_year_value) if previous_year_value.startswith('(') and previous_year_value.endswith(')'): previous_year_value = previous_year_value[1:-1] match = re.search(european_number_regex, str(previous_year_value)) if match or previous_year_value == '': if previous_year_value != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') values_to_add.append(previous_year_value) if both_entities_found: if idx + 3 < len(fields_data): current_year_value_1 = fields_data[idx + 3] if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(current_year_value_1)): current_year_value_1 = \"\" print(f\"Current Year Value: {current_year_value_1}\") if pd.isna(current_year_value_1) or current_year_value_1 == '-' or current_year_value_1 == 'nan': current_year_value_1 = '' values_to_add.append(current_year_value_1) # Ensure current_year_value is always a string before further processing current_year_value_1 = str(current_year_value_1) # Check and process values with parentheses if current_year_value_1.startswith('(') and current_year_value_1.endswith(')'): current_year_value_1 = current_year_value_1[1:-1] # Remove brackets # Match the value against the regex match = re.search(european_number_regex, current_year_value_1) if match or current_year_value_1 == '': if current_year_value_1 != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') values_to_add.append(current_year_value_1) print(\"yayyyy\", current_year_value_1) if idx + 4 < len(fields_data): previous_year_value_1 = fields_data[idx + 4] if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(previous_year_value_1)): previous_year_value_1 = \"\" print(f\"Previous Year Value: {previous_year_value_1}\") if pd.isna(previous_year_value_1) or previous_year_value_1 == '-' or previous_year_value_1 == 'nan': previous_year_value_1 = '' values_to_add.append(previous_year_value_1) # Ensure current_year_value is always a string before further processing previous_year_value_1 = str(previous_year_value_1) # if isinstance(previous_year_value, float): # value = str(previous_year_value) # Convert float to string for consistent processing if previous_year_value_1.startswith('(') and previous_year_value_1.endswith(')'): previous_year_value_1 = previous_year_value_1[1:-1] match = re.search(european_number_regex, str(previous_year_value_1)) if match or previous_year_value_1 == '': if previous_year_value_1 != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') values_to_add.append(previous_year_value_1) new_tags_list.extend(values_to_add) # Update the main dictionary and append it to the list if main_dict_node is not None: new_tags_dict[main_dict_node] = new_tags_list all_tags_data.append(new_tags_dict.copy()) # Use `.copy()` to avoid overwriting print(\"new_tags_dict\", new_tags_dict) # Perform additional operations if needed output, excel_nodes_list = compare_openai_and_Excel_data(node_list, fields_data_new, main_dict_node, both_entities_found, is_standalone_first) remove_duplicate_nodes_from_final_output(output, config_df, fields_data_new, is_pnl) # Print final list of all tags data print(\"All tags data:\", all_tags_data) # remove_nodes_from_openai_data(output, main_dict_node, excel_nodes_list) else: print(f\"No, financial values found for '{header}' in the two-year columns.\") row_start = header_row_index row_end_1 = len(fields_data) print(\"row_start_1\", row_start) print(\"row_end_1\", row_end_1) # Initialize row_end_2 to a default value row_end_2 = len(fields_data) potential_row_end_2 = [] # List to hold possible row_end_2 values greater than row_start # Find row_end_2 values based on ignore_headers for other_header in ignore_headers: print(\"other_header in ignore_headers\", other_header) for idx, field in enumerate(fields_data): if str(other_header).lower() == preprocess_field(field).lower(): print(f\"Potential row_end_2 found at row {idx} for header '{other_header}'\") potential_row_end_2.append(idx) break # Filter potential_row_end_2 to get the smallest value greater than row_start potential_row_end_2 = [val for val in potential_row_end_2 if val > row_start] if potential_row_end_2: row_end_2 = min(potential_row_end_2) # Smallest row_end_2 > row_start else: row_end_2 = len(fields_data) # Default to the last row if no valid row_end_2 is found print(\"row_start_2\", row_start) print(\"row_end_2\", row_end_2) # Determine the final row_end value if row_end_2 > row_start: row_end = row_end_2 else: row_end = row_end_1 print(\"final row_end\", row_end) fields_data_new = fields_data[row_start:row_end] print(\"fields_data_new\", fields_data_new) for idx, field in enumerate(fields_data_new): print('floAT',field) if ( preprocess_field(field) and all(node.lower() != preprocess_field(field).lower() for node in node_list) and preprocess_field(field).lower() not in (ignore.lower() for ignore in fields_to_ignore) and # Not in ignore list not re.search(date_regex, preprocess_field(field)) and # Field doesn't match date format not re.search(european_number_regex, preprocess_field(field)) and # Field doesn't match European number format not re.match(parentheses_number_regex, preprocess_field(field)) and not pd.isna(preprocess_field(field)) and # Field shouldn't be NaN \"unnamed\" not in preprocess_field(field).lower() and # Field shouldn't preprocess_field(field).lower() not in (ignore.lower() for ignore in fields_to_ignore_1) and not re.match(r\"^\\d{4}$\", preprocess_field(field).strip()) ): values_to_add = [preprocess_field(field)] print(\"Values_to_add\", values_to_add) # Check bounds to avoid IndexError if idx + 1 < len(fields_data_new): current_year_value = fields_data_new[idx + 1] print(\"current_year_value\", current_year_value) if pd.isna(current_year_value) or current_year_value == '-' or current_year_value == 'nan': current_year_value = '' values_to_add.append(current_year_value) # Ensure current_year_value is a string current_year_value = str(current_year_value) # Process parentheses if current_year_value.startswith('(') and current_year_value.endswith(')'): current_year_value = current_year_value[1:-1] # Match regex match = re.search(european_number_regex, current_year_value) if match or current_year_value == '': if current_year_value != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') values_to_add.append(current_year_value) if idx + 2 < len(fields_data_new): previous_year_value = fields_data_new[idx + 2] print(\"previous_year_value\", previous_year_value) if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(previous_year_value)): previous_year_value = \"\" print(f\"Previous Year Value: {previous_year_value}\") if pd.isna(previous_year_value) or previous_year_value == '-' or previous_year_value == 'nan': previous_year_value = '' values_to_add.append(previous_year_value) # Ensure previous_year_value is a string previous_year_value = str(previous_year_value) # Process parentheses if previous_year_value.startswith('(') and previous_year_value.endswith(')'): previous_year_value = previous_year_value[1:-1] match = re.search(european_number_regex, previous_year_value) if match or previous_year_value == '': if previous_year_value != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace( '#', ',') values_to_add.append(previous_year_value) if both_entities_found: if idx + 3 < len(fields_data): current_year_value_1 = fields_data[idx + 3] if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(current_year_value_1)): current_year_value_1 = \"\" print(f\"Current Year Value: {current_year_value_1}\") if pd.isna(current_year_value_1) or current_year_value_1 == '-' or current_year_value_1 == 'nan': current_year_value_1 = '' values_to_add.append(current_year_value_1) # Ensure current_year_value is always a string before further processing current_year_value_1 = str(current_year_value_1) # Check and process values with parentheses if current_year_value_1.startswith('(') and current_year_value_1.endswith(')'): current_year_value_1 = current_year_value_1[1:-1] # Remove brackets # Match the value against the regex match = re.search(european_number_regex, current_year_value_1) if match or current_year_value_1 == '': if current_year_value_1 != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') values_to_add.append(current_year_value_1) print(\"yayyyy\", current_year_value_1) if idx + 4 < len(fields_data): previous_year_value_1 = fields_data[idx + 4] if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', str(previous_year_value_1)): previous_year_value_1 = \"\" print(f\"Previous Year Value: {previous_year_value_1}\") if pd.isna(previous_year_value_1) or previous_year_value_1 == '-' or previous_year_value_1 == 'nan': previous_year_value_1 = '' values_to_add.append(previous_year_value_1) # Ensure current_year_value is always a string before further processing previous_year_value_1 = str(previous_year_value_1) # if isinstance(previous_year_value, float): # value = str(previous_year_value) # Convert float to string for consistent processing if previous_year_value_1.startswith('(') and previous_year_value_1.endswith(')'): previous_year_value_1 = previous_year_value_1[1:-1] match = re.search(european_number_regex, str(previous_year_value_1)) if match or previous_year_value_1 == '': if previous_year_value_1 != '': # formatted_value = match.group(0).replace('.', '#').replace(',', '.').replace('#', ',') values_to_add.append(previous_year_value_1) new_tags_list.extend(values_to_add) # Update and append the dictionary if main_dict_node is not None: new_tags_dict[main_dict_node] = new_tags_list all_tags_data.append(new_tags_dict.copy()) # Append a copy of the current dict print(\"new_tags_dict\", new_tags_dict) # Additional operations output, excel_nodes_list = compare_openai_and_Excel_data(node_list, fields_data_new, main_dict_node, both_entities_found, is_standalone_first) remove_duplicate_nodes_from_final_output(output, config_df, fields_data_new, is_pnl) # Print the final list of all tags data print(\"All tags data:\", all_tags_data) # remove_nodes_from_openai_data(output, main_dict_node, excel_nodes_list) return all_tags_data for entity in output.keys(): # Check if the entity has data if len(output[entity]) > 0: print(f\"Processing data for {entity}...\") # Get the years from the data (assuming they are in the format '31.12.22' and '31.12.21') years = list(output[entity][0].keys()) # print(\"years\",years) # Assume the first year in the list is the current year, and the second is the previous year current_year = years[0] # print(\"current_year\",current_year) previous_year = None if len(years)>1: previous_year = years[1] # print(\"previous_year\", previous_year) # Define the lists for each main_dict_node non_current_assets_list = [] current_assets_list = [] equity_list = [] non_current_liabilities_list = [] current_liabilities_list = [] total_assets_list = None total_liabilities_list = None # Iterate through the financial config DataFrame and segregate nodes for _, row in config_df.iterrows(): node = row['Node'] # print(\"node\",node) main_dict_node = row['main_dict_node'] node_type = row['Type'] # Check the Type column column_name = row['Column_Name'] # Append nodes only if Type is \"Straight\" and Node is not \"Null\" if node_type == 'Straight' and node != \"Null\": if main_dict_node == 'Non-current assets': non_current_assets_list.append(node) elif main_dict_node == 'Current assets': current_assets_list.append(node) elif main_dict_node == 'EQUITY': equity_list.append(node) elif main_dict_node == 'Non-current liabilities': non_current_liabilities_list.append(node) elif main_dict_node == 'Current liabilities': current_liabilities_list.append(node) total_assets_list = [\"TOTAL ACTIVITIES\", \"Total assets\", \"Activity\"] total_liabilities_list = [\"Total Net Equity and Liabilities\", \"Total LiabiLities and sharehoLders' Equity\", \"Total shareholders' equity and liabilities\", \"Shareholders' equity and liabilities\", \"Total liabilities\"] # Check nodes in all lists, with the appropriate headers to ignore for each non_current_assets_ignore_headers = current_assets_headers + equity_related_headers + non_current_liabilities_headers + current_liabilities_headers current_assets_ignore_headers = non_current_assets_headers + equity_related_headers + non_current_liabilities_headers + current_liabilities_headers equity_ignore_headers = non_current_assets_headers + current_assets_headers + non_current_liabilities_headers + current_liabilities_headers + total_assets_headers + total_liabilities_headers non_current_liabilities_ignore_headers = non_current_assets_headers + current_assets_headers + equity_related_headers + current_liabilities_headers current_liabilities_ignore_headers = non_current_assets_headers + current_assets_headers + equity_related_headers + non_current_liabilities_headers total_assets_ignore_headers = non_current_assets_headers + current_assets_headers + equity_related_headers + non_current_liabilities_headers total_liabilities_ignore_headers = non_current_assets_headers + current_assets_headers + equity_related_headers + non_current_liabilities_headers main_sections = [ { \"headers\": non_current_assets_headers, \"node_list\": non_current_assets_list, \"ignore_headers\": non_current_assets_ignore_headers, \"main_dict_node\": \"Non-current assets\", \"is_pnl\": False }, { \"headers\": current_assets_headers, \"node_list\": current_assets_list, \"ignore_headers\": current_assets_ignore_headers, \"main_dict_node\": \"Current assets\", \"is_pnl\": False }, { \"headers\": equity_related_headers, \"node_list\": equity_list, \"ignore_headers\": equity_ignore_headers, \"main_dict_node\": \"EQUITY\", \"is_pnl\": False }, { \"headers\": non_current_liabilities_headers, \"node_list\": non_current_liabilities_list, \"ignore_headers\": non_current_liabilities_ignore_headers, \"main_dict_node\": \"Non-current liabilities\", \"is_pnl\": False }, { \"headers\": current_liabilities_headers, \"node_list\": current_liabilities_list, \"ignore_headers\": current_liabilities_ignore_headers, \"main_dict_node\": \"Current liabilities\", \"is_pnl\": False }, { \"headers\": total_assets_headers, \"node_list\": total_assets_list, \"ignore_headers\": total_assets_ignore_headers, \"main_dict_node\": None, \"is_pnl\": False }, { \"headers\": total_liabilities_headers, \"node_list\": total_liabilities_list, \"ignore_headers\": total_liabilities_ignore_headers, \"main_dict_node\": None, \"is_pnl\": False } ] # Create an empty list to store DataFrames from all sheets sheets_to_concat = [] keywords_to_skip = [\"Adjustments for:\", \"cash flows\", \"TOTAL EQUITY - OPENING\", \"TOTAL EQUITY - CLOSING\", \"Balance at\", \"Dividends on ordinary shares\"] # Function to determine if a column name represents a year def is_year_column(column_name): # Assuming year columns are in the format \"DD/MM/YYYY\" or similar return re.match(r\"\\d{2}/\\d{2}/\\d{4}\", str(column_name)) # Process each sheet in the Excel file for sheet_name in excel_sheets.sheet_names: # print(f\"Processing sheet: {sheet_name}\") # Read the sheet into a DataFrame excel_df = excel_sheets.parse(sheet_name, header=0) # Check if any of the keywords are present in the sheet if excel_df.astype(str).apply( lambda col: col.str.contains('|'.join(keywords_to_skip), case=False, na=False)).any().any(): print(f\"Skipping sheet: {sheet_name} due to presence of keywords\") continue # Remove the \"Nota\" column (case-insensitive) from the DataFrame excel_df = excel_df.loc[:, ~excel_df.columns.str.lower().str.contains('note', na=False)] # Drop columns where any cell contains '[Subtotal]' (case-insensitive) excel_df = excel_df.loc[:, ~excel_df.apply( lambda col: col.astype(str).str.contains(r'\\[subtotal\\]', case=False, na=False) | col.astype(str).str.contains(\"of which due to related parties\", case=False, na=False) | col.astype(str).str.contains(\"NOTE\", case=False, na=False) ).any()] # Get all columns that represent years year_columns = [col for col in excel_df.columns if is_year_column(col)] # Check if there are exactly 3 year columns if len(year_columns) == 3: # Sort the year columns by date year_columns_sorted = sorted(year_columns, key=lambda x: pd.to_datetime(x, format=\"%d/%m/%Y\")) # Keep only the latest 2 years years_to_keep = year_columns_sorted[-2:] # Last 2 columns years_to_remove = year_columns_sorted[:-2] # Columns to remove # Drop the oldest year column from the DataFrame excel_df = excel_df.drop(columns=years_to_remove) print(f\"Removed column(s): {years_to_remove}\") else: pass # print(\"The DataFrame does not contain exactly 3 year columns.\") # Include the headers as the first row in the DataFrame header_row = pd.DataFrame([excel_df.columns.tolist()], columns=excel_df.columns) excel_df.columns = range(len(excel_df.columns)) # Temporarily reset column names to prevent conflicts excel_df = pd.concat([header_row, excel_df], ignore_index=True) # Append the DataFrame to the list of DataFrames sheets_to_concat.append(excel_df) # Concatenate all sheets into a single DataFrame appended_data = pd.concat(sheets_to_concat, ignore_index=True) # Create a DataFrame from the appended data excel_data_frame = pd.DataFrame(appended_data) # Print the final DataFrame print(\"excel_data_frame\", excel_data_frame) all_tags_data = [] # Initialize once before the loop if not is_pnl: for section in main_sections: all_tags_data = check_nodes_in_excel( section[\"headers\"], section[\"node_list\"], section[\"ignore_headers\"], section[\"main_dict_node\"], section[\"is_pnl\"], all_tags_data ) else: # Extract the list of PnL nodes pnl_fields_df = config_df[ config_df['Column_Name'].str.contains('financials_pnl_lineitems', case=False, na=False) ] pnl_fields_list = [ row['Node'] for _, row in pnl_fields_df.iterrows() if row['Type'] == 'Straight' and row['Node'] != \"Null\" ] # Define the headers and ignore headers for PnL pnl_headers = [\"profit and loss statement\", \"income statement\", \"profit and loss\"] pnl_ignore_headers = ( non_current_assets_headers + current_assets_headers + equity_related_headers + non_current_liabilities_headers + current_liabilities_headers ) # Call the function for PnL fields all_tags_data = check_nodes_in_excel( pnl_headers, pnl_fields_list, pnl_ignore_headers, None, is_pnl=True, all_tags_data = None) # Save updated output to JSON after processing all sheets with open(json_file_path, 'w') as json_file: json.dump(output, json_file, indent=4) print(f\"Updated data saved to {json_file_path}\") return output, all_tags_data output={ 'Group': [ { \"2024-03-31\": { \"Property, Plant and Equipment\": \"55161913\", \"Intangible Assets\": \"1837800\", \"Financial Assets \": \"\", \"Investment properties\": \"12830323\", \"Investments in subsidiaries\": \"\", \"Investment in Equity Accounted Investee\": \"6699920\", \"Other Investments\": \"\", \"Investments in associates\": \"\", \"Investment in preference shares\": \"\", \"Investment in joint ventures\": \"\", \"Deferred Tax Assets\": \"\", \"Null\": \"\", \"Loans and Advances\": \"\", \"Right of use assets\": \"20396191\", \"Lease Rentals Receivable\": \"\", \"Employee benefit plan asset\": \"\", \"Other receivables\": \"\", \"Other non financial assets\": \"\", \"Investment in unit trust\": \"\", \"Short Term Investments\": \"\", \"Inventories\": \"23283570\", \"Inventory\": \"\", \"Trade Receivables\": \"\", \"Receivable From Suppliers\": \"\", \"Other Receivables\": \"13054052\", \"Amount due from related companies\": \"\", \"Amounts Due From Related Parties\": \"\", \"Cash and cash equivalents\": \"8141729\", \"Cash and Short - Term Deposits\": \"\", \"Income tax receivables\": \"\", \"Tax Recoverable\": \"\", \"Assets held for Sale\": \"\", \"Other Current Financial Assets\": \"131107\", \"Securities Purchased under Repurchase Agr\": \"\", \"Total Assets\": \"143455510\", \"Stated Capital\": \"6489758\", \"Capital Reserves\": \"\", \"Fair Value Reserve\": \"\", \"Non Controlling Interest\": \"13731722\", \"Other Components of Equity\": \"\", \"Reserves\": \"6604113\", \"Reserves_of_a_disposal_group_held_for_sale\": \"\", \"Retained Earnings\": \"17060717\", \"Revaluation Reserve\": \"\", \"Revenue Reserves\": \"\", \"Interest Bearing Borrowings\": \"25414298\", \"Interest bearing liabilities\": \"\", \"Interest bearing loans and borrowings\": \"29932575\", \"Long term borrowings\": \"\", \"Deferred Tax Liabilities\": \"3567442\", \"Lease liabilities\": \"\", \"Loans from Related Party \": \"\", \"Retirement benefit liability\": \"2093066\", \"Retirement Benefit Obligation\": \"\", \"Unfunded retirement benefit obligation\": \"\", \"Employee Benefits Liabilities\": \"\", \"Short Term Interest Bearing Borrowings\": \"\", \"Bank Overdraft\": \"\", \"Trade Payables\": \"32669876\", \"Trade Creditors\": \"\", \"Other Payables\": \"\", \"Amount due to related companies\": \"\", \"Amounts Due to Related Parties\": \"\", \"Current Income Tax Payable\": \"\", \"Current Tax Liabilities\": \"5485759\", \"Income Tax Liabilities\": \"\", \"Income tax payables\": \"\", \"Contract liability\": \"\", \"Current portion of interest bearing liabilities\": \"\", \"Dividend payable\": \"208978\", \"Loan due to related companies\": \"\", \"Unclaimed dividends\": \"\", \"Warranty Provision\": \"\", \"Total Equity and Liabilities\": \"143455510\" } }, { \"2023-03-31\": { \"Property, Plant and Equipment\": \"48661880\", \"Intangible Assets\": \"1786183\", \"Financial Assets \": \"\", \"Investment properties\": \"9865959\", \"Investments in subsidiaries\": \"\", \"Investment in Equity Accounted Investee\": \"5879645\", \"Other Investments\": \"\", \"Investments in associates\": \"\", \"Investment in preference shares\": \"\", \"Investment in joint ventures\": \"\", \"Deferred Tax Assets\": \"22357\", \"Null\": \"\", \"Loans and Advances\": \"\", \"Right of use assets\": \"20696456\", \"Lease Rentals Receivable\": \"\", \"Employee benefit plan asset\": \"\", \"Other receivables\": \"\", \"Other non financial assets\": \"\", \"Investment in unit trust\": \"\", \"Short Term Investments\": \"\", \"Inventories\": \"22892606\", \"Inventory\": \"\", \"Trade Receivables\": \"\", \"Receivable From Suppliers\": \"\", \"Other Receivables\": \"10248128\", \"Amount due from related companies\": \"\", \"Amounts Due From Related Parties\": \"\", \"Cash and cash equivalents\": \"5072429\", \"Cash and Short - Term Deposits\": \"\", \"Income tax receivables\": \"\", \"Tax Recoverable\": \"\", \"Assets held for Sale\": \"\", \"Other Current Financial Assets\": \"119667\", \"Securities Purchased under Repurchase Agr\": \"\", \"Total Assets\": \"127234563\", \"Stated Capital\": \"6489758\", \"Capital Reserves\": \"\", \"Fair Value Reserve\": \"\", \"Non Controlling Interest\": \"12249407\", \"Other Components of Equity\": \"\", \"Reserves\": \"2956528\", \"Reserves_of_a_disposal_group_held_for_sale\": \"\", \"Retained Earnings\": \"13667618\", \"Revaluation Reserve\": \"\", \"Revenue Reserves\": \"\", \"Interest Bearing Borrowings\": \"27927579\", \"Interest bearing liabilities\": \"\", \"Interest bearing loans and borrowings\": \"25802377\", \"Long term borrowings\": \"\", \"Deferred Tax Liabilities\": \"1727522\", \"Lease liabilities\": \"\", \"Loans from Related Party \": \"\", \"Retirement benefit liability\": \"1927193\", \"Retirement Benefit Obligation\": \"\", \"Unfunded retirement benefit obligation\": \"\", \"Employee Benefits Liabilities\": \"\", \"Short Term Interest Bearing Borrowings\": \"\", \"Bank Overdraft\": \"\", \"Trade Payables\": \"28172892\", \"Trade Creditors\": \"\", \"Other Payables\": \"\", \"Amount due to related companies\": \"\", \"Amounts Due to Related Parties\": \"\", \"Current Income Tax Payable\": \"\", \"Current Tax Liabilities\": \"5957767\", \"Income Tax Liabilities\": \"\", \"Income tax payables\": \"\", \"Contract liability\": \"\", \"Current portion of interest bearing liabilities\": \"\", \"Dividend payable\": \"166063\", \"Loan due to related companies\": \"\", \"Unclaimed dividends\": \"\", \"Warranty Provision\": \"\", \"Total Equity and Liabilities\": \"127234563\" } } ], \"Company\": [ { \"2024-03-31\": { \"Property, Plant and Equipment\": \"2093\", \"Intangible Assets\": \"688467\", \"Financial Assets \": \"\", \"Investment properties\": \"1045044\", \"Investments in subsidiaries\": \"1625586\", \"Investment in Equity Accounted Investee\": \"5605950\", \"Other Investments\": \"\", \"Investments in associates\": \"\", \"Investment in preference shares\": \"\", \"Investment in joint ventures\": \"\", \"Deferred Tax Assets\": \"\", \"Null\": \"\", \"Loans and Advances\": \"\", \"Right of use assets\": \"76\", \"Lease Rentals Receivable\": \"\", \"Employee benefit plan asset\": \"\", \"Other receivables\": \"\", \"Other non financial assets\": \"\", \"Investment in unit trust\": \"\", \"Short Term Investments\": \"\", \"Inventories\": \"\", \"Inventory\": \"\", \"Trade Receivables\": \"\", \"Receivable From Suppliers\": \"\", \"Other Receivables\": \"42807\", \"Amount due from related companies\": \"\", \"Amounts Due From Related Parties\": \"\", \"Cash and cash equivalents\": \"457558\", \"Cash and Short - Term Deposits\": \"\", \"Income tax receivables\": \"\", \"Tax Recoverable\": \"\", \"Assets held for Sale\": \"\", \"Other Current Financial Assets\": \"\", \"Securities Purchased under Repurchase Agr\": \"\", \"Total Assets\": \"9549273\", \"Stated Capital\": \"6489758\", \"Capital Reserves\": \"\", \"Fair Value Reserve\": \"\", \"Non Controlling Interest\": \"\", \"Other Components of Equity\": \"\", \"Reserves\": \"91399\", \"Reserves_of_a_disposal_group_held_for_sale\": \"\", \"Retained Earnings\": \"2268764\", \"Revaluation Reserve\": \"\", \"Revenue Reserves\": \"\", \"Interest Bearing Borrowings\": \"401559\", \"Interest bearing liabilities\": \"\", \"Interest bearing loans and borrowings\": \"20\", \"Long term borrowings\": \"\", \"Deferred Tax Liabilities\": \"\", \"Lease liabilities\": \"\", \"Loans from Related Party \": \"\", \"Retirement benefit liability\": \"49474\", \"Retirement Benefit Obligation\": \"\", \"Unfunded retirement benefit obligation\": \"\", \"Employee Benefits Liabilities\": \"\", \"Short Term Interest Bearing Borrowings\": \"\", \"Bank Overdraft\": \"\", \"Trade Payables\": \"39321\", \"Trade Creditors\": \"\", \"Other Payables\": \"\", \"Amount due to related companies\": \"\", \"Amounts Due to Related Parties\": \"\", \"Current Income Tax Payable\": \"\", \"Current Tax Liabilities\": \"\", \"Income Tax Liabilities\": \"\", \"Income tax payables\": \"\", \"Contract liability\": \"\", \"Current portion of interest bearing liabilities\": \"\", \"Dividend payable\": \"208978\", \"Loan due to related companies\": \"\", \"Unclaimed dividends\": \"\", \"Warranty Provision\": \"\", \"Total Equity and Liabilities\": \"9549273\" } }, { \"2023-03-31\": { \"Property, Plant and Equipment\": \"2154\", \"Intangible Assets\": \"688467\", \"Financial Assets \": \"\", \"Investment properties\": \"748241\", \"Investments in subsidiaries\": \"1604677\", \"Investment in Equity Accounted Investee\": \"5605950\", \"Other Investments\": \"\", \"Investments in associates\": \"\", \"Investment in preference shares\": \"\", \"Investment in joint ventures\": \"\", \"Deferred Tax Assets\": \"\", \"Null\": \"\", \"Loans and Advances\": \"\", \"Right of use assets\": \"96\", \"Lease Rentals Receivable\": \"\", \"Employee benefit plan asset\": \"\", \"Other receivables\": \"\", \"Other non financial assets\": \"\", \"Investment in unit trust\": \"\", \"Short Term Investments\": \"\", \"Inventories\": \"\", \"Inventory\": \"\", \"Trade Receivables\": \"\", \"Receivable From Suppliers\": \"\", \"Other Receivables\": \"61671\", \"Amount due from related companies\": \"\", \"Amounts Due From Related Parties\": \"\", \"Cash and cash equivalents\": \"208338\", \"Cash and Short - Term Deposits\": \"\", \"Income tax receivables\": \"\", \"Tax Recoverable\": \"\", \"Assets held for Sale\": \"\", \"Other Current Financial Assets\": \"\", \"Securities Purchased under Repurchase Agr\": \"\", \"Total Assets\": \"9010349\", \"Stated Capital\": \"6489758\", \"Capital Reserves\": \"\", \"Fair Value Reserve\": \"\", \"Non Controlling Interest\": \"\", \"Other Components of Equity\": \"\", \"Reserves\": \"100462\", \"Reserves_of_a_disposal_group_held_for_sale\": \"\", \"Retained Earnings\": \"1497933\", \"Revaluation Reserve\": \"\", \"Revenue Reserves\": \"\", \"Interest Bearing Borrowings\": \"604120\", \"Interest bearing liabilities\": \"\", \"Interest bearing loans and borrowings\": \"94622\", \"Long term borrowings\": \"\", \"Deferred Tax Liabilities\": \"\", \"Lease liabilities\": \"\", \"Loans from Related Party \": \"\", \"Retirement benefit liability\": \"37621\", \"Retirement Benefit Obligation\": \"\", \"Unfunded retirement benefit obligation\": \"\", \"Employee Benefits Liabilities\": \"\", \"Short Term Interest Bearing Borrowings\": \"\", \"Bank Overdraft\": \"\", \"Trade Payables\": \"19770\", \"Trade Creditors\": \"\", \"Other Payables\": \"\", \"Amount due to related companies\": \"\", \"Amounts Due to Related Parties\": \"\", \"Current Income Tax Payable\": \"\", \"Current Tax Liabilities\": \"\", \"Income Tax Liabilities\": \"\", \"Income tax payables\": \"\", \"Contract liability\": \"\", \"Current portion of interest bearing liabilities\": \"\", \"Dividend payable\": \"166063\", \"Loan due to related companies\": \"\", \"Unclaimed dividends\": \"\", \"Warranty Provision\": \"\", \"Total Equity and Liabilities\": \"9010349\" } } ] } excel_file_path=r\"C:\\Users\\BRADSOL\\Documents\\srilanka\\C T HOLDINGS PLC\\split_C T HOLDINGS PLC.xlsx\" config_file_path=r\"C:\\Users\\BRADSOL\\Documents\\GitHub\\MNS-Srilanaka\\Config\\Financial_Config.xlsx\" json_file_path=r\"C:\\Users\\BRADSOL\\Documents\\srilanka\\C T HOLDINGS PLC\\split_C T HOLDINGS PLC.json\" is_pnl=False r=srilanka_mapping_and_comp(output, excel_file_path, config_file_path, json_file_path, is_pnl) print(r)"
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "The script you've shared seems to be a comprehensive process for mapping financial statement data extracted from Excel files to an output dictionary structure, which is then saved as a JSON file. Let’s break down the key components and functionality of your script to understand its flow and purpose:\n\n1. **Imports:**\n   - The script uses `pandas` for data manipulation, `re` for regular expression operations, and `json` for handling JSON files.\n\n2. **Function Definition:**\n   - The function `srilanka_mapping_and_comp` is defined to perform mapping from Excel data to a structured JSON output.\n\n3. **Regular Expressions:**\n   - Several regular expressions are defined at the beginning for matching numbers in a European format, dates, and parenthesized numbers.\n\n4. **Data Loading:**\n   - Configuration and Excel files are loaded using `pandas`. The configuration file appears to contain mapping details for nodes (financial line items).\n\n5. **Utility Functions:**\n   - Functions like `safe_clean_field`, `preprocess_field`, and others help in cleaning and preparing data fields for processing.\n\n6. **Main Process:**\n   - The script processes multiple sheets from the Excel file, ignoring specific keywords and columns. It looks for financial data tags (nodes) based on a configuration file and attempts to match them with data in the Excel sheets.\n   - It also determines whether both consolidated and standalone data are present and processes them accordingly.\n\n7. **Data Comparison:**\n   - The script compares extracted Excel data against data that might have been pre-existing in the `output` dictionary using the functions `compare_openai_and_Excel_data` and `remove_duplicate_nodes_from_final_output`.\n\n8. **Output Update:**\n   - After processing the Excel data and performing any necessary updates or mapping, the output dictionary is serialized to a JSON file using the `json` module.\n\n9. **Execution:**\n   - The function is called with specific paths for input Excel and JSON files along with a configuration file and an output dictionary.\n\nThis script is coordinated to handle financial data mapping and comparison in a structured manner, ideal for transforming financial line items from Excel to JSON by leveraging configuration-based mapping rules. You’d run this script in a Python environment with the necessary files located at the specified paths.\n\nIf you have any specific questions or require assistance in understanding a portion of the code better, feel free to ask!"
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "i m getting this error\"C:\\Users\\BRADSOL\\AppData\\Local\\Programs\\Python\\Python313\\python.exe C:\\Users\\BRADSOL\\Documents\\GitHub\\MNS-Srilanaka\\Srilanka_mapping_and_comparison.py \nitalian mapping an comp called\nis_pnl False\nProcessing data for Group...\nexcel_data_frame      For the year ended 31 March  ... controlling Non- interest\n0    For the year ended 31 March  ...                       NaN\n1                            NaN  ...                       NaN\n2                            NaN  ...                       NaN\n3                            NaN  ...                       NaN\n4                            NaN  ...                       NaN\n..                           ...  ...                       ...\n115                          NaN  ...                       NaN\n116                          NaN  ...                       NaN\n117                          NaN  ...                       NaN\n118                          NaN  ...                       NaN\n119                          NaN  ...                       NaN\n\n[120 rows x 26 columns]\nmain_dict_node Non-current assets and nodes_list [nan, 'Property, Plant and Equipment', nan, 'Intangible Assets', nan, nan, nan, 'Financial Assets ', nan, nan, nan, nan, 'Investment properties', 'Investments in subsidiaries', 'Investment in Equity Accounted Investee', 'Other Investments', 'Investments in associates', 'Investment in preference shares', 'Investment in joint ventures', 'Deferred Tax Assets', 'Loans and Advances', 'Right of use assets', nan, 'Lease Rentals Receivable', 'Employee benefit plan asset', 'Other receivables', 'Other non financial assets']\nBoth standalone and consolidated entities found.\nIndexes of standalone synonyms: 1\nIndexes of consolidated synonyms: 1\nMainnn_dict_node Non-current assets\nindices [993, 1279, 1305, 1461, 1487, 1513, 1669, 1695, 1851, 1877, 2007, 2033, 2059]\nFirst field in fields_to_ignore: non-current assets and row_start 993\nLast field in fields_to_ignore: total equity and liabilities and row_end 2060\nfields_dataaaaaaaaaaaaaaa ['non-current assets', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'property, plant and equipment', '55,161,913', '48,661,880', '2,093', '2,154', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'right of use of assets', '20,396,191', '20,696,456', '76', '96', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'prepayment on leasehold land and building', '-', '200,912', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'investment property', '12,830,323', '9,865,959', '1,045,044', '748,241', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'intangible assets', '1,837,800', '1,786,183', '688,467', '688,467', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'investments in subsidiaries', '-', '-', '1,625,586', '1,604,677', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'investment in equity accounted investees', '6,699,920', '5,879,645', '5,605,950', '5,605,950', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'other financial assets', '1,703,150', '1,730,787', '81,692', '90,755', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'deferred tax assets', '-', '22,357', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'biological assets', '65,737', '23,873', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'total non-current assets', '98,695,034', '88,868,052', '9,048,908', '8,740,340', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'current assets', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'inventories', '23,283,570', '22,892,606', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'biological assets', '150,018', '33,681', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'trade and other receivables', '13,054,052', '10,248,128', '42,807', '61,671', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'other financial assets', '131,107', '119,667', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'cash and cash equivalents', '8,141,729', '5,072,429', '457,558', '208,338', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'total current assets', '44,760,476', '38,366,511', '500,365', '270,009', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'total assets', '143,455,510', '127,234,563', '9,549,273', '9,010,349', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'equity', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'stated capital', '6,489,758', '6,489,758', '6,489,758', '6,489,758', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'reserves', '6,604,113', '2,956,528', '91,399', '100,462', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'retained earnings', '17,060,717', '13,667,618', '2,268,764', '1,497,933', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'total equity attributable to equity holders of the parent', '30,154,588', '23,113,904', '8,849,921', '8,088,153', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'non-controlling interest', '13,731,722', '12,249,407', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'total equity', '43,886,310', '35,363,311', '8,849,921', '8,088,153', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'liabilities', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'non - current liabilities interest bearing loans and borrowings', '25,414,298', '27,927,579', '401,559', '604,120', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'deferred tax liabilities', '3,567,442', '1,727,522', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'employee benefits', '2,093,066', '1,927,193', '49,474', '37,621', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'capital grants', '-', '4,493', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'trade and other payables', '197,206', '185,366', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'total non-current liabilities', '31,272,012', '31,772,153', '451,033', '641,741', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'current liabilities', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'trade and other payables', '32,669,876', '28,172,892', '39,321', '19,770', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'current tax liabilities', '5,485,759', '5,957,767', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'dividend payable', '208,978', '166,063', '208,978', '166,063', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'interest bearing loans and borrowings', '29,932,575', '25,802,377', '20', '94,622', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'total current liabilities', '68,297,188', '60,099,099', '248,319', '280,455', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'total liabilities', '99,569,200', '91,871,252', '699,352', '922,196', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'total equity and liabilities']\nHeader 'Non-current assets' found at index 0 in fields_data.\nValue 1: nan\nValue 2: nan\nfinancial_values []\nNo, financial values found for 'Non-current assets' in the two-year columns.\nrow_start_1 0\nrow_end_1 1067\nother_header in ignore_headers Current assets\nPotential row_end_2 found at row 312 for header 'Current assets'\nother_header in ignore_headers Current activities\nother_header in ignore_headers EQUITY\nPotential row_end_2 found at row 520 for header 'EQUITY'\nother_header in ignore_headers Non-current liabilities\nother_header in ignore_headers Non current liabilities\nother_header in ignore_headers Current liabilities\nPotential row_end_2 found at row 884 for header 'Current liabilities'\nrow_start_2 0\nrow_end_2 312\nfinal row_end 312\nfields_data_new ['non-current assets', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'property, plant and equipment', '55,161,913', '48,661,880', '2,093', '2,154', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'right of use of assets', '20,396,191', '20,696,456', '76', '96', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'prepayment on leasehold land and building', '-', '200,912', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'investment property', '12,830,323', '9,865,959', '1,045,044', '748,241', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'intangible assets', '1,837,800', '1,786,183', '688,467', '688,467', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'investments in subsidiaries', '-', '-', '1,625,586', '1,604,677', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'investment in equity accounted investees', '6,699,920', '5,879,645', '5,605,950', '5,605,950', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'other financial assets', '1,703,150', '1,730,787', '81,692', '90,755', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'deferred tax assets', '-', '22,357', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'biological assets', '65,737', '23,873', '-', '-', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'total non-current assets', '98,695,034', '88,868,052', '9,048,908', '8,740,340', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan']\nfloAT non-current assets\nTraceback (most recent call last):\n  File \"C:\\Users\\BRADSOL\\Documents\\GitHub\\MNS-Srilanaka\\Srilanka_mapping_and_comparison.py\", line 1515, in <module>\n    r=srilanka_mapping_and_comp(output, excel_file_path, config_file_path, json_file_path, is_pnl)\n  File \"C:\\Users\\BRADSOL\\Documents\\GitHub\\MNS-Srilanaka\\Srilanka_mapping_and_comparison.py\", line 1153, in srilanka_mapping_and_comp\n    all_tags_data = check_nodes_in_excel(\n        section[\"headers\"],\n    ...<4 lines>...\n        all_tags_data\n    )\n  File \"C:\\Users\\BRADSOL\\Documents\\GitHub\\MNS-Srilanaka\\Srilanka_mapping_and_comparison.py\", line 854, in check_nodes_in_excel\n    all(node.lower() != preprocess_field(field).lower() for node in node_list) and\n    ~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\BRADSOL\\Documents\\GitHub\\MNS-Srilanaka\\Srilanka_mapping_and_comparison.py\", line 854, in <genexpr>\n    all(node.lower() != preprocess_field(field).lower() for node in node_list) and\n        ^^^^^^^^^^\nAttributeError: 'float' object has no attribute 'lower'\n\nProcess finished with exit code 1\n\""
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "The error message you're encountering indicates an `AttributeError` occurring in your script, specifically when the code is trying to call the `lower()` method on a `float` object. This issue arises because not all elements in `node_list` are strings; some of them are numbers (as floats).\n\n### Steps to Resolve the Error\n\n1. **Ensure Proper Field Conversion:**\n   - Before attempting to convert a node to lowercase, ensure it is a string. You can modify the list comprehension to check and convert only string elements to lowercase.\n\n2. **Update the Affected Code:**\n\nHere is the updated portion of the `check_nodes_in_excel` function, where the error occurs:\n\n```python\nall(isinstance(node, str) and node.lower() != preprocess_field(field).lower() for node in node_list) and\n```\n\nBy using `isinstance(node, str)`, we ensure that `node.lower()` is only being called on elements that are strings, thus avoiding the `AttributeError` when processing float or NaN values.\n\n### Explanation and Additional Tips\n\n- **NaN Handling:** When processing Excel data, NaN values can sometimes propagate through the DataFrame. Using methods like `dropna()` early in your data cleaning step can help reduce NaN issues.\n- **Consistent Data Types:** Ensure that any non-string data is either filtered out or treated appropriately to avoid issues with string operations (like calling `.lower()`).\n- **Debugging:** Add debug prints, like `print(type(node))`, to help identify unexpected data types causing errors. Logging can help track down such issues during runtime.\n\nMake these adjustments and re-run your script to check if the issue resolves. If other parts of the code also handle strings, it might be useful to implement similar checks and conversions throughout. If more errors persist, similar checks for types and operations should be applied elsewhere as needed."
        }
      ]
    }
  ],
  response_format={
    "type": "text"
  },
  temperature=1,
  max_completion_tokens=2048,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0
)